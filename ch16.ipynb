{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76e28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590effc5",
   "metadata": {},
   "source": [
    "# Generating Shakespearean Text Using a Character RNN\n",
    "## Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b73791",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\" # shortcut URL\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b73a79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "939272e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ed9707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c700fb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "print(max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "084dae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "dataset_size = tokenizer.document_count # total number of characters\n",
    "print(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a989bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1 # Minus one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aee5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset for training set\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222650de",
   "metadata": {},
   "source": [
    "## Chopping the Sequential Dataset into Multiple Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87b4d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3750f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e7bb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9caa699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "                lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a65ac92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92850c5f",
   "metadata": {},
   "source": [
    "## Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "155cd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "# history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88ad6b",
   "metadata": {},
   "source": [
    "## Using the Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02145b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fdc7fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benson516/ml/my_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff883e",
   "metadata": {},
   "source": [
    "## Generating Fake Shakespearean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93a0265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80d5d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c12e6520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t,$fvp-$a\n",
      "zwp;wznl'qo?yzb3m!h,-kx.,'k:p,k?x3;'$ekg.\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecd3cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcdae!io:njogbphzxje&du\n",
      "avpu\n",
      " zmq$djvtbdcvadq ;y!mq\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41911d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t !$wo'kc,twb\n",
      "$3&-v,fy;&b.r onjmmr'gs; ,n;3b\n",
      "!eoqm&\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8949de",
   "metadata": {},
   "source": [
    "## Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cedcda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "                      lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c75bb56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                    dropout=0.2, recurrent_dropout=0.2,\n",
    "                    batch_input_shape=[1, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95c1770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a738f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "# model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a821f991",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67ddc059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imdb review dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4166a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_+3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a47e666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /home/benson516/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099d0d66606648b583a790a624f05477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5695c6cc736f412091d2a45fea89de11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling imdb_reviews-train.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling imdb_reviews-test.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling imdb_reviews-unsupervised.tfrecord...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /home/benson516/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a13d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12b9f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "658130e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214462), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2214c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54eb471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c34d2afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "006463f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9795129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33b691a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 645s 813ms/step - loss: 0.6851 - accuracy: 0.5326\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 640s 818ms/step - loss: 0.4498 - accuracy: 0.7876\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 624s 798ms/step - loss: 0.2920 - accuracy: 0.8827\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 676s 864ms/step - loss: 0.1778 - accuracy: 0.9354\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 696s 890ms/step - loss: 0.1388 - accuracy: 0.9510\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                          input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "             metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c120616",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e90bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "model = keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29e5de",
   "metadata": {},
   "source": [
    "## Reusing PretrainedEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7e81a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b9722a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 25s 29ms/step - loss: 0.5489 - accuracy: 0.7219\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.5135 - accuracy: 0.7490\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.5084 - accuracy: 0.7510\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.5046 - accuracy: 0.7543\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.5020 - accuracy: 0.7561\n"
     ]
    }
   ],
   "source": [
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc6816",
   "metadata": {},
   "source": [
    "# An Encoder-Decoder Network for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04239b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, \n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state, \n",
    "    sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "                    outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83137b44",
   "metadata": {},
   "source": [
    "# Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "487cb7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.wrappers.Bidirectional at 0x7f3872bb6b70>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da488686",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad92b051",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-369f6a0cd9fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     encoder_state, multiplier=beam_width)\n\u001b[1;32m      6\u001b[0m output, _, _ = decoder(\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0membedding_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     initial_state=decoder_initial_state)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_decoder' is not defined"
     ]
    }
   ],
   "source": [
    "beam_width = 10\n",
    "decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
    "    cell=decoder_cell, beam_width=beam_width, output_layer=output_layer)\n",
    "decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(\n",
    "    encoder_state, multiplier=beam_width)\n",
    "output, _, _ = decoder(\n",
    "    embedding_decoder, start_tokens=start_tokens, end_token=end_token,\n",
    "    initial_state=decoder_initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc61376",
   "metadata": {},
   "source": [
    "# Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90cba848",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-44083a1e339d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(\n\u001b[0;32m----> 2\u001b[0;31m     units, encoder_state, memory_sequence_length=encoder_sequence_length)\n\u001b[0m\u001b[1;32m      3\u001b[0m attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(\n\u001b[1;32m      4\u001b[0m     decoder_cell, attention_mechanism, attention_layer_size=n_units)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'units' is not defined"
     ]
    }
   ],
   "source": [
    "attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(\n",
    "    units, encoder_state, memory_sequence_length=encoder_sequence_length)\n",
    "attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(\n",
    "    decoder_cell, attention_mechanism, attention_layer_size=n_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df81e5",
   "metadata": {},
   "source": [
    "## Attention is All You Need: The TRansformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeaa05f",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "031db0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2]  = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_encoding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_encoding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5aaf2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer of the transformer\n",
    "embed_size = 512\n",
    "max_steps = 500\n",
    "vocab_size = 10000\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "\n",
    "embedings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afebcc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = encoder_in\n",
    "for N in range(6):\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for N in range(6):\n",
    "    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
    "    \n",
    "outputs = keras.layers.TimeDistributed(\n",
    "    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5eab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2820e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee636c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68110636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ace034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
